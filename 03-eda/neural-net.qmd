---
title: "Neural-net"
author: "Dusty Turner"
format: html
engines:
  R: true
  python: true
cache: true
warning: false
message: false
---

## Testing

```{r}
library(tidyverse)
library(tidymodels)
library(reticulate)
source(here::here("03-eda", "data_cleaning.R"))
```

```{r}

deff <-
defensive_model_building_data %>% 
  filter(is.na(pass_result)) |>
  filter(frame_id >= 5) |> 
  ## finds frames from tackle
  group_by(game_id,play_id, nfl_id) |> 
  # filter(cur_group_id() == 1) |> 
  mutate(row_with_tackle = mean(ifelse(event == "tackle", cur_group_rows(), NA ), na.rm = TRUE)) |> 
  mutate(frames_from_tackle = cur_group_rows() - row_with_tackle) |> 
  ungroup() |> 
  filter(frames_from_tackle <=0) |> 
  select(-row_with_tackle) |> 
  ## end finds frames from tackle
  mutate(position = as.character(position)) %>%
  mutate(position = replace_na(position, "unknown")) %>%
  mutate(position = factor(position)) |> 
  mutate(offense_formation = if_else(is.na(offense_formation), "unk", offense_formation)) |> 
  mutate(alignment_cluster = if_else(is.na(alignment_cluster), "unk", alignment_cluster)) |> 
  select(c(x,y,distance_to_ball, distance_to_ball_next, x_going, y_going, s, a, o, dir, x_ball, y_ball, x_ball_next, y_ball_next, s_ball, o_ball, dir_ball, angle_to_ball,
           position, offense_formation, quarter, down, rank,
           defenders_in_the_box, ball_in_fan, pass_probability, yards_to_go, x_from_los, height, weight, tackle, frames_from_tackle, game_id, play_id, nfl_id, frame_id, display_name, game_idplay_id)) 

```

```{python}
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.model_selection import GroupShuffleSplit
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.layers import LSTM

# Load and preprocess data
data = r.deff

# Impute missing numerical values with mean
numerical_features = ['x', 'y', 'distance_to_ball', 'distance_to_ball_next', 'x_going', 'y_going', 's', 'a', 'o', 'dir', 'x_ball', 'y_ball', 'x_ball_next', 'y_ball_next', 'pass_probability', 'yards_to_go', 'x_from_los', 'rank', 'height', 'weight', 's_ball', 'angle_to_ball']
for col in numerical_features:
    if data[col].isnull().any():
        data[col].fillna(data[col].mean(), inplace=True)

# Impute missing categorical values with mode (or a placeholder like 'missing')
categorical_features = ['ball_in_fan', 'position', 'offense_formation', 'quarter', 'down']
for col in categorical_features + ['frame_id']:
    if data[col].isnull().any():
        data[col].fillna(data[col].mode()[0], inplace=True)

data.replace([np.inf, -np.inf], np.nan, inplace=True)  # Handle infinite values

# Define feature categories
numerical_features = ['x', 'y', 'distance_to_ball', 'distance_to_ball_next', 'x_going', 'y_going', 's', 'a', 'o', 'dir', 'x_ball', 'y_ball', 'x_ball_next', 'y_ball_next', 'pass_probability', 'yards_to_go', 'x_from_los', 'rank', 'height', 'weight', 's_ball', 'angle_to_ball']
categorical_features = ['ball_in_fan', 'position', 'offense_formation', 'quarter', 'down']

# Set up preprocessing pipeline
preprocessor = ColumnTransformer(
    transformers=[
        ('num', Pipeline([
            ('imputer', SimpleImputer(strategy='mean')),
            ('scaler', StandardScaler())
        ]), numerical_features),
        ('cat', Pipeline([
            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
            ('onehot', OneHotEncoder(handle_unknown='ignore'))
        ]), categorical_features)
    ]
)

# Split data based on 'game_id'
group_column = 'game_id'
target_column = 'tackle'
additional_columns = ['game_id', 'play_id', 'nfl_id', 'frame_id', 'frames_from_tackle', 'display_name']  # Specify additional columns to keep

gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=0)
for train_idx, test_idx in gss.split(data, data[target_column], data[group_column]):
    train_data = data.iloc[train_idx]
    test_data = data.iloc[test_idx]

# Store additional columns and then drop them from the feature set
additional_columns_train = train_data[additional_columns].copy()
additional_columns_test = test_data[additional_columns].copy()

X_train = train_data.drop([target_column] + additional_columns, axis=1)
y_train = train_data[target_column].astype(int)
X_test = test_data.drop([target_column] + additional_columns, axis=1)
y_test = test_data[target_column].astype(int)

# Build the model
def build_model(input_shape):
    model = Sequential([
        Dense(64, activation='relu', input_shape=[input_shape], kernel_regularizer=l2(0.001)),
        Dropout(0.3),
        Dense(64, activation='relu', kernel_regularizer=l2(0.001)),
        Dropout(0.3),
        Dense(64, activation='tanh', kernel_regularizer=l2(0.001)),
        Dropout(0.3),
        Dense(1, activation='sigmoid', kernel_regularizer=l2(0.001))  # Apply L2 regularization here
    ])
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

from tensorflow.keras.layers import BatchNormalization

def build_model(input_shape):
    model = Sequential([
        Dense(64, activation='relu', input_shape=[input_shape], kernel_regularizer=l2(0.001)),
        BatchNormalization(),  # Add Batch Normalization here
        Dropout(0.3),
        Dense(64, activation='relu', kernel_regularizer=l2(0.001)),
        BatchNormalization(),  # Add Batch Normalization here
        Dropout(0.3),
        Dense(64, activation='tanh', kernel_regularizer=l2(0.001)),
        BatchNormalization(),  # Add Batch Normalization here
        Dropout(0.3),
        Dense(1, activation='sigmoid', kernel_regularizer=l2(0.001))  # Apply L2 regularization here
    ])
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model



# def build_model(input_shape, lstm_units=64):
#     model = Sequential([
#         Masking(mask_value=0.0, input_shape=(None, input_shape)),  # Use None for variable sequence length
#         LSTM(lstm_units, return_sequences=True),  # LSTM layer with return_sequences=True
#         Dropout(0.1),
#         LSTM(lstm_units),  # LSTM layer without return_sequences
#         Dropout(0.1),
#         Dense(1, activation='sigmoid')
#     ])
#     model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
#     return model


# Preprocess the data
X_train_transformed = preprocessor.fit_transform(X_train)
X_test_transformed = preprocessor.transform(X_test)

# Convert the target variable to numpy array if it's a pandas Series
y_train_np = y_train.values if isinstance(y_train, pd.Series) else y_train
y_test_np = y_test.values if isinstance(y_test, pd.Series) else y_test

# Now build and train the model
model = build_model(X_train_transformed.shape[1])

callbacks = [
    EarlyStopping(monitor='val_loss', patience=10, verbose=1),
    ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)
]

# Train the model with callbacks
history = model.fit(X_train_transformed, y_train_np, epochs=7, validation_split=0.2, callbacks=callbacks)

# After making predictions
predictions = model.predict(X_test_transformed)
predicted_classes = (predictions > 0.5).astype(int)

# Evaluate the model
evaluation = model.evaluate(X_test_transformed, y_test_np)
print("Test loss:", evaluation[0])
print("Test accuracy:", evaluation[1])


```



```{r}

test_set_with_predictions <-
  as_tibble(py$X_test) %>% 
  mutate(tackle = as.factor(py$y_test), 
         pred_class = as.factor(py$predicted_classes),
         pred = py$predictions[,1],
         game_id = py$additional_columns_test$game_id,
         play_id = py$additional_columns_test$play_id,
         nfl_id = py$additional_columns_test$nfl_id,
         frame_id = py$additional_columns_test$frame_id,
         frames_from_tackle = py$additional_columns_test$frames_from_tackle,
         display_name = py$additional_columns_test$display_name)  |>
  # filter(frames_from_tackle >= -40) |> 
  relocate(game_id, play_id, pred, frames_from_tackle, tackle)  

accuracy_over_time <-
test_set_with_predictions |> 
  group_by(frames_from_tackle) |>
  yardstick::accuracy(truth = tackle, pred_class) |> 
  ggplot(aes(x = frames_from_tackle, y = .estimate)) +
  geom_line()

animated_plot <-
week_1 |> 
  left_join(test_set_with_predictions |> select(game_id, play_id, nfl_id, frame_id, pred)) |> 
  filter(!is.na(pred)) |> 
  group_by(game_id, play_id) |> filter(cur_group_id() == 1) |> ungroup() |> 
  ggplot(aes(x = frame_id, y = pred)) + 
  geom_line() +
  geom_point(show.legend = FALSE, size = 3) + # Add a point for each line
  facet_wrap(~jersey_number) +
  transition_reveal(frame_id) # Animate the points along the line

anm1 <- animate(animated_plot, width = 800, height = 600, nframes = 100)

animated_plot2 <-
week_1 |> 
  filter(play_id %in% test_set_with_predictions$play_id) |> 
  filter(game_id %in% test_set_with_predictions$game_id) |> 
  left_join(
    test_set_with_predictions |> select(game_id, play_id, nfl_id, frame_id, pred) |> 
      mutate(pred = ifelse(is.na(pred), 0, pred))
    ) |> 
  group_by(game_id, play_id) |> 
  filter(cur_group_id() == 1) |>
  mutate(pred = ifelse(club == defensive_team & is.na(pred), 0, pred)) |> 
  ungroup()  |> 
  mutate(color = pred) |> 
  mutate(jersey_number = ifelse(defensive_team == club, jersey_number, "")) |> 
  select(distance_to_ball, x, y, color, absolute_yardline_number, ball_carrier, play_id, time, play_description, is_football, club, jersey_number, defensive_team) %>%
{
  ggplot(data = ., aes(x = x, y = y, color = color)) +
  geom_vline(aes(xintercept = absolute_yardline_number), color = "blue") +
  geom_point(aes(shape = is_football), size = 3, show.legend = FALSE) +
  geom_text(aes(label = jersey_number), color = "black", nudge_y = -1) +
  scale_color_gradient(low = "grey", high = "black", na.value = "dodgerblue") +
  transition_time(time) + ease_aes("linear") +
  labs(y = "", x = "Yards To Endzone", title = str_wrap(.$play_description[1], width = 80)) +
  theme_field
}

anm2 <- animate(animated_plot2, width = 800, height = 600, nframes = 100)

accuracy <- py$evaluation[[2]]

nn_briar <- 
  test_set_with_predictions |> 
  select(game_id, play_id, nfl_id, display_name,tackle, pred) |> 
  filter(!is.na(pred)) |> 
  mutate(tackle  = as.integer(as.character(tackle))) |> 
  group_by(game_id, play_id, nfl_id, display_name) |> 
  reframe(expected_prob_of_tackle = mean(pred), tackle = mean(tackle)) |> 
  mutate(tackles_over_expected_play = ifelse(tackle == 1, 1-expected_prob_of_tackle, -expected_prob_of_tackle)) |> 
  group_by(nfl_id, display_name) |> 
  reframe(tackles_over_expected = sum(tackles_over_expected_play)) |> 
  arrange(-tackles_over_expected)

list(nn_anim1 = anm1, nn_anim2 = anm2, accuracy = accuracy, accuracy_over_time = accuracy_over_time, nn_briar = nn_briar) |> 
  write_rds(file = "99-addm/nn.RDS")

```

```{r}
preds_in_r <-
as_tibble(py$X_test) |> 
  mutate(tackle = as.factor(py$y_test)) |> 
  mutate(pred = as.factor(py$predicted_classes))



```


## This works on work computer

```{r}
# library(reticulate)
# Sys.setenv(RETICULATE_PYTHON = "C:/Users/Dusty_Turner1/AppData/Local/Programs/Python/Python311/python.exe")
# options(reticulate.use_condaenv = FALSE)
# use_python("C:/Users/Dusty_Turner1/AppData/Local/Programs/Python/Python311/python.exe", required = TRUE)
# "C:/Users/Dusty_Turner1/AppData/Local/Programs/Python/Python311/python.exe" -m pip install pandas

```

```{python}
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Load and preprocess data
# Assuming r.deff is your DataFrame loaded previously
data = r.deff  
data.replace([np.inf, -np.inf], np.nan, inplace=True)  # Handle infinite values

# Define feature categories
numerical_features = ['x', 'y', 'distance_to_ball', 'distance_to_ball_next', 'x_going', 'y_going', 's', 'a', 'o', 'dir', 'x_ball', 'y_ball',  'x_ball_next', 'y_ball_next', 'pass_probability', 'yards_to_go', 'x_from_los', 'rank', 'height', 'weight', 's_ball', 'o_ball', 'dir_ball', 'angle_to_ball']
categorical_features = ['ball_in_fan', 'position', 'offense_formation', 'quarter', 'down']

# Set up preprocessing pipeline
preprocessor = ColumnTransformer(
    transformers=[
        ('num', Pipeline([
            ('imputer', SimpleImputer(strategy='mean')),
            ('scaler', StandardScaler())
        ]), numerical_features),
        ('cat', Pipeline([
            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
            ('onehot', OneHotEncoder(handle_unknown='ignore'))
        ]), categorical_features)
    ]
)

# Split data
X = data.drop('tackle', axis=1)
y = data['tackle'].astype(int)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Build the model
def build_model(input_shape):
    model = Sequential([
        Dense(64, activation='relu', input_shape=[input_shape]),
        Dense(64, activation='relu'),
        Dense(64, activation='relu'),
        Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

model = build_model(X_train_transformed.shape[1])

# Create the pipeline
pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('model', model)
])

# Train the pipeline
pipeline = Pipeline([('preprocessor', preprocessor), ('model', build_model(preprocessor.fit_transform(X_train).shape[1]))])
history = pipeline.fit(X_train, y_train, model__epochs=7, model__validation_split=0.2)

# Evaluate model
test_loss, test_accuracy = pipeline.named_steps['model'].evaluate(pipeline.named_steps['preprocessor'].transform(X_test), y_test, verbose=2)
print(f"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}")

```

## Make Predictions

```{python}
# Transform the test data using the preprocessor in the pipeline
X_test_transformed = pipeline.named_steps['preprocessor'].transform(X_test)

# Use the model to make predictions on the transformed test data
predictions = pipeline.named_steps['model'].predict(X_test_transformed)

# Since predictions will be in the form of probabilities (due to the sigmoid activation), 
# you might want to convert these to a binary form (0 or 1) based on a threshold (usually 0.5)
predicted_classes = (predictions > 0.5).astype(int)

# predicted_classes now holds the vector of predictions (0 or 1) for the test set

```

```{r}
library(reticulate)
py$X_test |> as_tibble() |> 
  mutate(tackle = as.factor(py$y_test), prediction = py$predictions[,1], class = as.factor(py$predicted_classes[,1])) |> 
  select(tackle, prediction, class) |> 
  yardstick::accuracy(truth = tackle, estimate = class)
```

## lstm attempt

```{python}
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.model_selection import GroupShuffleSplit
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, LSTM
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Load and preprocess data
data = r.deff

# Define numerical and categorical features
numerical_features = ['x', 'y', 'distance_to_ball', 'distance_to_ball_next', 'x_going', 'y_going', 's', 'a', 'o', 'dir', 'x_ball', 'y_ball', 'x_ball_next', 'y_ball_next', 'pass_probability', 'yards_to_go', 'x_from_los', 'rank', 'height', 'weight', 's_ball', 'angle_to_ball']
categorical_features = ['ball_in_fan', 'position', 'offense_formation', 'quarter', 'down']

# Impute missing numerical values with mean
for col in numerical_features:
    if data[col].isnull().any():
        data[col].fillna(data[col].mean(), inplace=True)

# Impute missing categorical values with mode (or a placeholder like 'missing')
for col in categorical_features + ['frame_id']:
    if data[col].isnull().any():
        data[col].fillna(data[col].mode()[0], inplace=True)
        
data.replace([np.inf, -np.inf], np.nan, inplace=True)

# Define preprocessing steps for numerical and categorical features
preprocessor = ColumnTransformer(
    transformers=[
        ('num', Pipeline([
            ('imputer', SimpleImputer(strategy='mean')),
            ('scaler', StandardScaler())
        ]), numerical_features),
        ('cat', Pipeline([
            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
            ('onehot', OneHotEncoder(handle_unknown='ignore'))
        ]), categorical_features)
    ]
)

# Function to extract feature names
def get_feature_names(column_transformer):
    output_features = []

    for name, pipe, features in column_transformer.transformers_:
        if name == 'num':
            output_features.extend(features)
        elif name == 'cat':
            for i in pipe.named_steps['onehot'].get_feature_names(input_features=features):
                output_features.append(i)

    return output_features

# Split data
group_column = 'game_idplay_id'
target_column = 'tackle'
additional_columns = ['game_id', 'play_id', 'nfl_id', 'frame_id', 'frames_from_tackle', 'game_idplay_id']

gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=0)
for train_idx, test_idx in gss.split(data, data[target_column], data[group_column]):
    train_data = data.iloc[train_idx]
    test_data = data.iloc[test_idx]

# Apply preprocessor to training and test data
X_train = train_data.drop([target_column] + additional_columns, axis=1)
X_test = test_data.drop([target_column] + additional_columns, axis=1)
X_train_transformed = preprocessor.fit_transform(X_train)
X_test_transformed = preprocessor.transform(X_test)

new_feature_names = get_feature_names(preprocessor)

# Create DataFrames with the new feature names
X_train_transformed_df = pd.DataFrame(X_train_transformed, columns=new_feature_names)
X_test_transformed_df = pd.DataFrame(X_test_transformed, columns=new_feature_names)

# Add the 'game_idplay_id' column back for grouping
train_data.reset_index(drop=True, inplace=True)
X_train_transformed_df['game_idplay_id'] = train_data['game_idplay_id']

# Group by 'game_idplay_id'
grouped_train = [group.drop(columns='game_idplay_id') for _, group in X_train_transformed_df.groupby('game_idplay_id')]

# Repeat for the test set
test_data.reset_index(drop=True, inplace=True)
X_test_transformed_df['game_idplay_id'] = test_data['game_idplay_id']
grouped_test = [group.drop(columns='game_idplay_id') for _, group in X_test_transformed_df.groupby('game_idplay_id')]

# Pad sequences
padded_sequences_train = pad_sequences([g.values for g in grouped_train], padding='post', dtype='float32')
padded_sequences_test = pad_sequences([g.values for g in grouped_test], padding='post', dtype='float32')

# Aggregate y_train and y_test to match the number of sequences based on 'game_idplay_id'
y_train_aggregated = train_data.groupby('game_idplay_id')[target_column].agg(lambda x: x.mode()[0]).astype(int).values
y_test_aggregated = test_data.groupby('game_idplay_id')[target_column].agg(lambda x: x.mode()[0]).astype(int).values

# Define LSTM model
def build_lstm_model(input_shape):
    model = Sequential([
        LSTM(64, return_sequences=True, input_shape=[None, input_shape], kernel_regularizer=l2(0.0005)),
        Dropout(0.1),
        LSTM(64, return_sequences=False, kernel_regularizer=l2(0.0005)),
        Dropout(0.1),
        Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

# Build the model
model = build_lstm_model(padded_sequences_train.shape[2])  # Adjust for the correct input shape

# Callbacks
callbacks = [
    EarlyStopping(monitor='val_loss', patience=10, verbose=1),
    ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)
]

# Train the model
history = model.fit(padded_sequences_train, y_train_aggregated, epochs=5, validation_split=0.2, callbacks=callbacks)

# Making predictions with the test set
predictions = model.predict(padded_sequences_test)
predicted_classes = (predictions > 0.5).astype(int)

# Evaluate the model
evaluation = model.evaluate(padded_sequences_test, y_test_aggregated)
print("Test loss:", evaluation[0])
print("Test accuracy:", evaluation[1])


# Assuming you have predictions and test_data DataFrames
predictions_df = pd.DataFrame({'predicted_class': predicted_classes.flatten(), 'pred': predictions.flatten()})
result_df = pd.concat([test_data, predictions_df], axis=1)


```

### back up pre lstm

```{python}
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.model_selection import GroupShuffleSplit
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.layers import LSTM

# Load and preprocess data
data = r.deff

# Impute missing numerical values with mean
numerical_features = ['x', 'y', 'distance_to_ball', 'distance_to_ball_next', 'x_going', 'y_going', 's', 'a', 'o', 'dir', 'x_ball', 'y_ball', 'x_ball_next', 'y_ball_next', 'pass_probability', 'yards_to_go', 'x_from_los', 'rank', 'height', 'weight', 's_ball', 'angle_to_ball']
for col in numerical_features:
    if data[col].isnull().any():
        data[col].fillna(data[col].mean(), inplace=True)

# Impute missing categorical values with mode (or a placeholder like 'missing')
categorical_features = ['ball_in_fan', 'position', 'offense_formation', 'quarter', 'down']
for col in categorical_features + ['frame_id']:
    if data[col].isnull().any():
        data[col].fillna(data[col].mode()[0], inplace=True)

data.replace([np.inf, -np.inf], np.nan, inplace=True)  # Handle infinite values

# Define feature categories
numerical_features = ['x', 'y', 'distance_to_ball', 'distance_to_ball_next', 'x_going', 'y_going', 's', 'a', 'o', 'dir', 'x_ball', 'y_ball', 'x_ball_next', 'y_ball_next', 'pass_probability', 'yards_to_go', 'x_from_los', 'rank', 'height', 'weight', 's_ball', 'angle_to_ball']
categorical_features = ['ball_in_fan', 'position', 'offense_formation', 'quarter', 'down']

# Set up preprocessing pipeline
preprocessor = ColumnTransformer(
    transformers=[
        ('num', Pipeline([
            ('imputer', SimpleImputer(strategy='mean')),
            ('scaler', StandardScaler())
        ]), numerical_features),
        ('cat', Pipeline([
            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
            ('onehot', OneHotEncoder(handle_unknown='ignore'))
        ]), categorical_features)
    ]
)

# Split data based on 'game_id'
group_column = 'game_id'
target_column = 'tackle'
additional_columns = ['game_id', 'play_id', 'nfl_id', 'frame_id', 'frames_from_tackle']  # Specify additional columns to keep

gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=0)
for train_idx, test_idx in gss.split(data, data[target_column], data[group_column]):
    train_data = data.iloc[train_idx]
    test_data = data.iloc[test_idx]

# Store additional columns and then drop them from the feature set
additional_columns_train = train_data[additional_columns].copy()
additional_columns_test = test_data[additional_columns].copy()

X_train = train_data.drop([target_column] + additional_columns, axis=1)
y_train = train_data[target_column].astype(int)
X_test = test_data.drop([target_column] + additional_columns, axis=1)
y_test = test_data[target_column].astype(int)

# Build the model
def build_model(input_shape):
    model = Sequential([
        Dense(64, activation='relu', input_shape=[input_shape], kernel_regularizer=l2(0.0005)),
        Dropout(0.1),
        Dense(64, activation='relu', kernel_regularizer=l2(0.0005)),
        Dropout(0.1),
        Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

# def build_model(input_shape, lstm_units=64):
#     model = Sequential([
#         Masking(mask_value=0.0, input_shape=(None, input_shape)),  # Use None for variable sequence length
#         LSTM(lstm_units, return_sequences=True),  # LSTM layer with return_sequences=True
#         Dropout(0.1),
#         LSTM(lstm_units),  # LSTM layer without return_sequences
#         Dropout(0.1),
#         Dense(1, activation='sigmoid')
#     ])
#     model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
#     return model


# Preprocess the data
X_train_transformed = preprocessor.fit_transform(X_train)
X_test_transformed = preprocessor.transform(X_test)

# Convert the target variable to numpy array if it's a pandas Series
y_train_np = y_train.values if isinstance(y_train, pd.Series) else y_train
y_test_np = y_test.values if isinstance(y_test, pd.Series) else y_test

# Now build and train the model
model = build_model(X_train_transformed.shape[1])

callbacks = [
    EarlyStopping(monitor='val_loss', patience=10, verbose=1),
    ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)
]

# Train the model with callbacks
history = model.fit(X_train_transformed, y_train_np, epochs=2, validation_split=0.2, callbacks=callbacks)

# After making predictions
predictions = model.predict(X_test_transformed)
predicted_classes = (predictions > 0.5).astype(int)

# Evaluate the model
evaluation = model.evaluate(X_test_transformed, y_test_np)
print("Test loss:", evaluation[0])
print("Test accuracy:", evaluation[1])


```
