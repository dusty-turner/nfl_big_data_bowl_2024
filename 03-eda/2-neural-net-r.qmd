---
title: "Neural-net"
author: "Dusty Turner"
format: html
engines:
  R: true
  python: true
cache: true
warning: false
message: false
---

## Testing

```{r}
library(tidyverse)
library(keras)
library(tensorflow)
library(reticulate)
# library(data.table)
# library(gganimate)
# library(arrow)

x_train <- read_rds("02-clean-data/x_train.rds")
y_train <- read_rds("02-clean-data/y_train.rds")
x_test <- read_rds("02-clean-data/x_test.rds")
y_test <- read_rds("02-clean-data/y_test.rds")
time_steps <- read_rds("02-clean-data/time_steps.rds")
num_features <- dim(x_train)[3]

CustomSaveModelCallback <- R6::R6Class(
  "CustomSaveModelCallback",
  inherit = KerasCallback,

  public = list(
    model = NULL,           
    best_val_loss = Inf,    
    best_model_path = NULL, 
    best_metrics = list(),  # Store the best metrics

    initialize = function(model) {
      self$model <- model
      self$best_metrics <- list(accuracy = 0, precision = 0, recall = 0, auc = 0, f1_score = 0, log_loss = Inf)
    },

    on_epoch_end = function(epoch, logs = list()) {
      val_loss <- logs[['val_loss']]
      if (!is.null(val_loss) && val_loss < self$best_val_loss) {
        # Update the best validation loss and metrics
        self$best_metrics$best_val_loss <- val_loss
        self$best_metrics$accuracy <- logs[['python_function']]
        self$best_metrics$precision <- logs[['precision']]
        self$best_metrics$recall <- logs[['recall']]
        self$best_metrics$auc <- logs[['auc']]
        # self$best_metrics$f1_score <- logs[['f1_score']]  # Update F1 score
        # self$best_metrics$log_loss <- logs[['log_loss']]  # Update log loss

        # Save model and update path
        if (!is.null(self$best_model_path) && file.exists(self$best_model_path)) {
          unlink(self$best_model_path, recursive = TRUE)
        }
        self$best_model_path <- sprintf("04-models/model-epoch-%02d-val_loss-%.2f", epoch, val_loss)
        save_model_tf(self$model, self$best_model_path)
      }
    }
  )
)


weighted_binary_crossentropy <- function(y_true, y_pred, positive_weight = 8.5) {
  # Calculate binary crossentropy
  bce <- keras::k_binary_crossentropy(y_true, y_pred)
  
  # Apply weights
  weights <- tf$cast(y_true, tf$float32) * (positive_weight - 1) + 1
  weighted_bce <- weights * bce
  
  # Return mean loss over the batch
  return(keras::k_mean(weighted_bce))
}

custom_accuracy <- function(y_true, y_pred) {
  # Apply threshold to get binary class predictions
  threshold <- 0.5
  y_pred_binary <- k_cast(k_greater_equal(y_pred, threshold), 'float32')
  
  # Calculate accuracy
  correct_predictions <- k_equal(y_pred_binary, y_true)
  return(k_mean(correct_predictions))
}

run_nn <- function(units_param = 400, rate_param = .5, l2_value_param = .001, epochs_param = 3, 
                   optimizer_param = 'adam', positive_weight_param = 8.5,
                   layers_param = 3, batch_size_param = 7028/8, notes_param = NULL){

  keras::backend()$clear_session()

  positive_weight <- positive_weight_param
  
  now_time <- now()
  formatted_time <- format(now_time, "%Y-%m-%d_%H-%M-%S")
  
  raw_tracker <-
    read_csv(here::here("04-models", "model_tracker.csv")) 
    raw_tracker |> 
    filter(epochs == epochs_param, units == units_param, rate == rate_param, 
           l2_value == l2_value_param,
           # activation == activation_param,
           optimizer == optimizer_param, layers == layers_param) %>%
      mutate(model_execution_time = as.character(model_execution_time)) |> 
    print()
    

model <- keras_model_sequential() %>%
  layer_masking(mask_value = 0, input_shape = c(time_steps, num_features)) %>%
  layer_lstm(units = 400, return_sequences = TRUE, unroll = FALSE,
             kernel_regularizer = regularizer_l2(l2_value_param),
             recurrent_regularizer = regularizer_l2(l2_value_param), activation = "relu") %>%
  layer_dropout(rate = rate_param) %>%
  # layer_lstm(units = 200, return_sequences = TRUE, unroll = FALSE,
  #            kernel_regularizer = regularizer_l2(l2_value_param),
  #            recurrent_regularizer = regularizer_l2(l2_value_param), activation = "relu") %>%
  # layer_dropout(rate = rate_param) %>%
  # layer_lstm(units = 200, return_sequences = TRUE, unroll = FALSE,
  #            kernel_regularizer = regularizer_l2(l2_value_param),
  #            recurrent_regularizer = regularizer_l2(l2_value_param), activation = "relu") %>%
  # layer_dropout(rate = rate_param) %>%
  layer_lstm(units = 200, return_sequences = TRUE, unroll = FALSE,
             kernel_regularizer = regularizer_l2(l2_value_param),
             recurrent_regularizer = regularizer_l2(l2_value_param), activation = "relu") %>%
  layer_dropout(rate = rate_param) %>%
  time_distributed(layer_dense(units = 11, activation = 'sigmoid'))

# Compile the model
model %>% compile(
  loss = weighted_binary_crossentropy,
  optimizer = optimizer_adam(learning_rate = 0.001),
  metrics = list(
    custom_accuracy,       # Assuming this is a predefined metric
    metric_precision(),    # Precision
    metric_recall(),       # Recall
    # f1_score,              # F1 Score (custom)
    metric_auc()          # AUC-ROC
    # log_loss               # Log Loss (custom)
  )
)

      # Define the multi-GPU strategy
  # strategy <- tf$distribute$MirroredStrategy()
  # 
  # with(strategy$scope(), {
  #   model <- keras_model_sequential() %>%
  #     layer_masking(mask_value = 0, input_shape = c(time_steps, num_features)) %>%
  #     layer_lstm(units = units_param, return_sequences = TRUE, unroll = FALSE,
  #                kernel_regularizer = regularizer_l2(l2_value_param),
  #                recurrent_regularizer = regularizer_l2(l2_value_param)) %>%
  #     layer_dropout(rate = rate_param) %>%
  #     layer_lstm(units = units_param, return_sequences = TRUE, unroll = FALSE,
  #                kernel_regularizer = regularizer_l2(l2_value_param),
  #                recurrent_regularizer = regularizer_l2(l2_value_param)) %>%
  #     layer_dropout(rate = rate_param) %>%
  #     time_distributed(layer_dense(units = 11, activation = 'sigmoid'))
  # 
  #   # Compile the model
  #   model %>% compile(
  #     loss = weighted_binary_crossentropy,
  #     optimizer = optimizer_adam(learning_rate = 0.001),
  #     metrics = list(
  #       custom_accuracy,       # Assuming this is a predefined metric
  #       metric_precision(),    # Precision
  #       metric_recall(),       # Recall
  #       # f1_score,              # F1 Score (custom)
  #       metric_auc()           # AUC-ROC
  #       # log_loss               # Log Loss (custom)
  #     )
  #   )
  # })
    
model_start_time <- tictoc::tic()

save_model_callback <- CustomSaveModelCallback$new(model)

# Train the model
history <- model %>% fit(
  x = x_train, y = y_train,
  epochs = epochs_param,
  batch_size = batch_size_param,
  validation_data = list(x_test, y_test),
  callbacks = list(
    # early_stop_callback,
    save_model_callback
    # reduce_lr_callback
  #   # tensorboard_callback,
  #   # csv_logger_callback
  )
)

 best_metrics <- save_model_callback$best_metrics

 save_model_callback$best_metrics
 
model_end_time <- tictoc::toc()
model_execution_time <- (model_end_time$toc - model_end_time$tic)[[1]]  # Calculate execution time for model building and training

# final_accuracy <- history$metrics$python_function[length(history$metrics$precision)]
# final_precision <- history$metrics$precision[length(history$metrics$precision)]
# final_recall <- history$metrics$recall[length(history$metrics$recall)]
# final_f1_score <- history$metrics$f1_score[length(history$metrics$f1_score)]
# final_auc <- history$metrics$auc[length(history$metrics$auc)]
# final_log_loss <- history$metrics$log_loss[length(history$metrics$log_loss)]
best_precision <- best_metrics$precision
best_recall <- best_metrics$recall
best_auc <- best_metrics$auc
best_log_loss <- best_metrics$best_val_loss 
best_accuracy <- best_metrics$accuracy
# best_f1_score <- best_metrics$f1_score 

tibble(
  accuracy = best_accuracy, precision = best_precision, recall = best_recall, 
  # f1_score = best_f1_score, 
  auc = best_auc, 
  log_loss = best_log_loss,epochs = epochs_param, units = units_param, layers = layers_param,l2_value = l2_value_param,
  optimizer = optimizer_param, rate = rate_param, time = now_time, user = Sys.info()["user"], 
  positive_weight_param = positive_weight_param,model_execution_time = model_execution_time, notes = notes_param
) |> 
    bind_rows(raw_tracker) |> 
  relocate(accuracy, log_loss) %>%
    arrange(desc(time)) |> 
    print() %>% 
    write_csv(here::here("04-models", "model_tracker.csv"))

# save_model_tf(model, str_c("04-models/", formatted_time))

return(list(model = model, history = history))  
  
}


# ratio <-
# newdata %>% 
#   count(tackle) %>% 
#   reframe(perc = n / sum(n)) %>% 
#   slice(1) %>% pull(perc) * 10

out <- run_nn(
  epochs_param = 10,
  units_param = 400,
  rate_param = .5,
  l2_value_param = .001,
  # l2_value_param = 0,
  layers_param = 4,
  batch_size_param = 7028/8,
  positive_weight_param = 8.5,
  notes_param = "3 400s and a 200 4 layers",
  optimizer_param = "adam"
)


epoch_values <- c(10000)
rate_values <- c(.5,.75)
l2_values <- c(.001)
positive_weight_value <- c(ratio)
units_value <- c(400)

# Generate all combinations
set.seed(123)
param_grid <- expand.grid(epochs_param = epoch_values,
                          rate_param = rate_values,
                          l2_value_param = l2_values,
                          units_param = units_value,
                          positive_weight_param = positive_weight_value)  

  # sample_n(35)

pmap(param_grid, function(epochs_param, rate_param, l2_value_param, units_param, positive_weight_param) {
  run_nn(
    epochs_param = epochs_param,
    units_param = units_param,  # Assuming a fixed value
    rate_param = rate_param,
    l2_value_param = l2_value_param,
    layers_param = 2,  # Assuming a fixed value
    batch_size_param = 7028,  # Assuming a fixed value
    notes_param = "Hyperparameter tuning 1111",
    positive_weight_param = positive_weight_param,
    optimizer_param = "adam"  # Assuming a fixed value
    # activation_param = "relu"  # Uncomment if needed
  )
})

# View the training history
plot(out$history) + xlim(0,430)
# history$metrics |> as_tibble() |> print(n = Inf)
```


```{r}
model2 <- load_model_tf("04-models/2024-01-07_16-03-36", custom_objects = list(
  # model2 <- load_model_tf("04-models", custom_objects = list(
    weighted_binary_crossentropy = weighted_binary_crossentropy,
    python_function = weighted_binary_crossentropy
))
```

### Accuracy


```{r}
# Predict on test data

y_pred_test <- out$model %>% predict(x_test)
y_pred_test <- model2 %>% predict(x_test)

# Binarize predictions (since we used a sigmoid activation, the output is a probability)
y_pred_binarized <- array(ifelse(y_pred_test > 0.5, 1, 0), dim(y_pred_test))

# Calculate accuracy
accuracy <- sum(y_pred_binarized == y_test) / length(y_test)

# Print accuracy
print(paste("Accuracy:", accuracy))

# Flatten the tracking arrays for the test set
tracking_test_flat <- array_reshape(do.call(cbind, tracking_test), c(dim(y_pred_test)[1] * dim(y_pred_test)[2] * dim(y_pred_test)[3]))

  # Flatten your model predictions in a similar manner
predictions_flat <- array_reshape(y_pred_test, c(dim(y_pred_test)[1] * dim(y_pred_test)[2] * dim(y_pred_test)[3]))

# Now, filter out padded predictions using the tracking data
final_predictions <- predictions_flat[tracking_test_flat == 1]


test_joined_with_preds <-
test_data |> 
  arrange(game_id, play_id, frame_id, display_name) |>
  mutate(game_id = as.character(game_id)) |> 
  mutate(play_id = as.character(play_id)) |> 
  mutate(tackle = as.factor(tackle)) |> 
  mutate(final_predictions = as.double(final_predictions)) |> 
  mutate(binary_predictions = as.factor(ifelse(final_predictions> .5, 1, 0))) |> 
  mutate(game_idplay_id = str_c(game_id, play_id))

test_joined_with_preds <- 
test_joined_with_preds %>% 
  select(game_id,play_id, frame_id, display_name, final_predictions, binary_predictions) %>% 
  left_join(week_1) %>% 
  mutate(tackle = as.factor(tackle)) 
  

test_joined_with_preds |> 
  count(binary_predictions)

test_joined_with_preds |> 
group_by(play_type) %>%
  yardstick::accuracy(truth = tackle, estimate = binary_predictions) %>%
  print(n = Inf)

test_joined_with_preds |> 
  group_by(play_type) %>%
  yardstick::precision(truth = tackle, estimate = binary_predictions) 

test_joined_with_preds |> 
  group_by(play_type) %>%
  yardstick::recall(truth = tackle, estimate = binary_predictions)


test_joined_with_preds |> 
  mutate(game_idplay_id = str_c(game_id,play_id)) %>% 
  # group_by(game_id, play_id) |>
  filter(game_idplay_id=="2022090800167") %>% 
  select(display_name, tackle, final_predictions, binary_predictions, jersey_number) |> 
  arrange(display_name) |> 
  print(n = Inf)
```

```{r}
# write_rds(test_joined_with_preds,"04-models/test_joined_with_preds_2024-01-07_16-03-36.rds")
```



```{r}

library(pROC)

# Creating the ROC object
roc_result<- roc(response = test_joined_with_preds$tackle, predictor = test_joined_with_preds$final_predictions)

# Calculating the AUC
auc_value <- auc(roc_result)

# Plotting the ROC curve
ggplot(data = data.frame(fpr = 1 - roc_result$specificities, tpr = roc_result$sensitivities), aes(x = fpr, y = tpr)) +
  geom_line() +
  geom_abline(linetype = "dashed") +
  labs(x = "False Positive Rate", y = "True Positive Rate", title = "ROC Curve") +
  annotate("text", x = 0.6, y = 0.4, label = paste("AUC =", round(auc_value, 2)))


```


## Animations

```{r}
library(gganimate)

# Assuming week_1 is your data frame
animated_plot <-
  week_1 %>% 
    filter(game_idplay_id == "2022090800167") %>%  
  left_join(test_joined_with_preds %>% select(game_id, play_id, display_name, frame_id, final_predictions), by = c("game_id", "play_id", "display_name", "frame_id")) %>% 
  filter(!is.na(final_predictions)) %>% 
  group_by(game_id, play_id) %>% 
  # filter(cur_group_id() == 4) |> pull(game_idplay_id)
  ungroup()  |> 
  select(frame_id, final_predictions, jersey_number, tackle) |>
  ggplot(aes(x = frame_id, y = final_predictions)) + 
  geom_line() +
  geom_point(show.legend = FALSE, size = 3) + 
  facet_wrap(~jersey_number) +
  transition_reveal(frame_id)

anm1 <- animate(animated_plot, width = 800, height = 600, nframes = 100)

animated_plot2 <-
week_1 |> 
  filter(game_idplay_id=="2022090800167") %>% 
  # filter(play_id %in% test_joined_with_preds$play_id) |> 
  # filter(game_id %in% test_joined_with_preds$game_id) |> 
  left_join(
    test_joined_with_preds |> select(game_id, play_id, display_name, frame_id, final_predictions) |> 
      mutate(final_predictions = ifelse(is.na(final_predictions), 0, final_predictions))
    ) |> 
  group_by(game_id, play_id) |> 
  # filter(cur_group_id() == 4) |>
  # filter(frame_id >=5) |> 
  filter(frame_id <= max(frame_id)-4) |>
  mutate(final_predictions = ifelse(club == defensive_team & is.na(final_predictions), 0, final_predictions)) |> 
  ungroup()  |> 
  # select(final_predictions) %>% filter(!is.na(final_predictions)) %>% print(n = Inf)
  mutate(color = final_predictions) |> 
  mutate(jersey_number = ifelse(defensive_team == club, jersey_number, "")) |> 
  select(display_name, distance_to_ball, x, y, color, absolute_yardline_number, ball_carrier, play_id, time, play_description, is_football, club, jersey_number, final_predictions, defensive_team, frame_id) %>%
  # filter(!is.na(final_predictions)) %>% pull(color) %>% range()
{
  ggplot(data = ., aes(x = x, y = y, color = color)) +
      # ggplot(aes(x = x, y = y, color = color)) +
  geom_vline(aes(xintercept = absolute_yardline_number), color = "blue") +
  geom_point(aes(shape = is_football), size = 3, show.legend = FALSE) +
  geom_text(aes(label = jersey_number), color = "black", nudge_y = -1) +
  scale_color_gradient(low = "grey", high = "black", na.value = "dodgerblue") +
  transition_time(time) + ease_aes("linear") +
  labs(y = "", x = "Yards To Endzone", title = str_wrap(.$play_description[1], width = 80)) +
  theme_field
}

anm2 <- animate(animated_plot2, width = 800, height = 600, nframes = 100)

# nn_briar <- 
  test_joined_with_preds |>
    mutate(game_idplay_id = str_c(game_id, play_id)) |> 
  select(game_idplay_id, display_name,tackle, final_predictions) |> 
  filter(!is.na(final_predictions)) |> 
  mutate(tackle  = as.integer(as.character(tackle))) |> 
  mutate(part = tackle - final_predictions)   |> 
  group_by(game_idplay_id, display_name)  |> 
  summarise(mean_error_play = mean(part)) |> 
  group_by(display_name) |> 
  summarise(average_tackles_over_expected = mean(mean_error_play), n = n()) |> 
  arrange(-average_tackles_over_expected) |> 
  filter(n >= 10)
    
  

```


